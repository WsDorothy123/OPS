//
// auto-generated by ops.py
//
#include "./OpenMP4/tea_leaf_common.h"

#define OPS_GPU

extern int xdim0_tea_leaf_ppcg_reduce_kernel;
extern int xdim1_tea_leaf_ppcg_reduce_kernel;
extern int xdim2_tea_leaf_ppcg_reduce_kernel;

#undef OPS_ACC0
#undef OPS_ACC1
#undef OPS_ACC2

#define OPS_ACC0(x, y) (x + xdim0_tea_leaf_ppcg_reduce_kernel * (y))
#define OPS_ACC1(x, y) (x + xdim1_tea_leaf_ppcg_reduce_kernel * (y))
#define OPS_ACC2(x, y) (x + xdim2_tea_leaf_ppcg_reduce_kernel * (y))

// user function

void tea_leaf_ppcg_reduce_kernel_c_wrapper(double *p_a0, int base0, int tot0,
                                           double *p_a1, int base1, int tot1,
                                           double *p_a2, int base2, int tot2,
                                           double *p_a3, int x_size,
                                           int y_size) {
  double p_a3_0 = p_a3[0];
#ifdef OPS_GPU

#pragma omp target teams distribute parallel for num_teams(OPS_threads)        \
    thread_limit(OPS_threads_for_block)                                        \
        schedule(static, 1) map(tofrom : p_a3_0) reduction(+ : p_a3_0)
#endif
  for (int i = 0; i < y_size * x_size; i++) {
    int n_x = i % x_size;
    int n_y = i / x_size;
    const double *rstore = p_a0 + base0 + n_x * 1 * 1 +
                           n_y * xdim0_tea_leaf_ppcg_reduce_kernel * 1 * 1;

    const double *r = p_a1 + base1 + n_x * 1 * 1 +
                      n_y * xdim1_tea_leaf_ppcg_reduce_kernel * 1 * 1;
    const double *z = p_a2 + base2 + n_x * 1 * 1 +
                      n_y * xdim2_tea_leaf_ppcg_reduce_kernel * 1 * 1;

    double *rnn = &p_a3_0;

    *rnn =
        *rnn + (r[OPS_ACC1(0, 0)] - rstore[OPS_ACC0(0, 0)]) * z[OPS_ACC2(0, 0)];
  }
  p_a3[0] = p_a3_0;
}
#undef OPS_ACC0
#undef OPS_ACC1
#undef OPS_ACC2
